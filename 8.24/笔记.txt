 爬虫的重点:
1.找接口
2.解析数据 --> 数据持久化

数据持久化:
1.csv (表格)

2.数据库 (mysql,oracle..)

3.json

数据库: 存储数据 (服务器)

服务器: web服务器 DNS服务器(域名解析)  数据库(数据存储服务)

DNS服务器: 将域名(https://后面的内容)解析为对应的ip地址

1.安装数据库环境(wamp,mamp: 集成了mysql的数据库环境)

2.前端(用户可以操作的)软件: 操作数据库的 (navicat for mysql)

wamp: windows里面的mysql环境的集成软件

mamp: mac里面的mysql环境的集成软件

安装成功以后,打开wamp/mamp:

windows: 绿色图标 (服务器开启)

mac: start services (开启mysql服务)

使用浏览器访问:
windows: localhost

mac: localhost:8888

使用数据库一定要保证wamp/mamp是开启状态

mac: 一直下一步安装即可(14天)

windows:
1.安装成功以后,桌面上出现绿色图标
2.打开PatchNavicat.exe --> 找到mysql的安装路径 --> 打开即可 
3.打开桌面上的图标

连接数据库环境:
1.新建连接
2.打开连接(绿色,说明成功)

数据库和数据库表的关系: 文件夹和excel文件的关系

excel的表头在数据库里面称之为字段

新建数据库(文件夹):
1.右击连接
2.新建数据库
3.数据库名随意,字符集(utf-8,utf8mb4),排序规则(utf8_general_ci,utf8mb4_general_ci)
utf8mb4: 当存储的数据有一些特殊字符的时候,如果使用utf-8会报错,但是使用utf8mb4不会报错

(*￣︶￣)
4.如果发现设置有误,右击-->数据库属性

data = [
    {'姓名': '张三','性别': '男','年龄': 20,'身高': 180},
    {'姓名': '张三','性别': '男','年龄': 18,'身高': 160},
    {'姓名': '王五','性别': '男','年龄': 23,'身高': 150},
    {'姓名': '赵六','性别': '女','年龄': 25,'身高': 190},
    {'姓名': '田七','性别': '男','年龄': 21,'身高': 170}
]

新建数据库表(excel文件)
1.双击数据库 --> 右击数据库里面的表 --> 新建表
2.关于字段(最好是英文),一定要有主键(自己设置,字段名一般是id或者带有id的单词)
主键: 唯一性 (区分每个数据,一般就是序号)
3.设置主键 (数字类型,钥匙,自动递增)
4.每个字段都会有对应的类型(接受什么样的数据)
	数字类型: 如果数据以后涉及到需要进行一些数值上的改变  (int)

	字符串类型: 如果数据以后不涉及到数值上的改变 (varchar: 0-255的长度 text: 超出255长度范围)
5.设置每个键值对对应的字段名
6.ctrl+s --> 输入表名(不要写中文,不要有特殊符号)
7.如果发现表的设置有问题(右击表,设计表,重新设计,最后一定要ctrl+s)
8.双击表,查看表里面的数据
9.修改了表的数据,一定要ctrl+s

如何操作数据库里面的数据(增删改查): sql(结构查询语言) 不区分大小写

查: 
	select * from 表名;
查看几条数据:
	select count(*) from 表名;

运行: ctrl(command) + r

增(插入数据): 
insert into 表名 (字段名1,字段名2,...) values (value1,value2,...);

删除: delete from 表名;


关于赋值的时候,字符串一定要打上单引号或者双引号(字段名不需要标点符号,值需要)

INSERT INTO people (name,sex,age,height) VALUES ('小明','男',10,180);


python操作数据库:
1.安装pymysql: pip install pymysql
2.保证数据库是开启(绿色图标)
3.连接数据库(数据库名,用户名,密码...)

如果要将爬虫的数据存储到数据库当中,先在数据库里面建立数据库表

a = "文字'哈哈哈"

"INSERT INTO zcool (name) values ('%s')"%(a)


"INSERT INTO zcool (name) values ('文字'哈哈哈')"

连接数据库:
db = pymysql.connect(主机名,登录用户,登录密码,连接的数据库,端口号)

db = pymysql.connect('localhost','root','','python',8889)


数据持久化-- json (文件后缀名:  xxx.json)

保存的格式:
{
	"data": [{},{},{},{}]
}

标准的json写法: 每个字符串的符号是双引号的

import json

1.写入json方式:
json.dumps(): 将json对象转化为字符串,再使用file.write()写入

json.dump(obj,file): 直接将json对象写入到file里面

2.读取json方式
json.loads(): 将文件的读取的内容(字符串) --> j


json.load(obj,file): 将json文件的内容读取出来,返回值是json对象(字典)



爬虫: 找接口 --> 请求头  --> 服务器的响应数据(text,content,json)  --> 数据解析 --> 数据持久化


假如有种操作,可以让我们看见什么样的网页,服务器就给我们返回对应网页的源代码!!!

所见及所得: selenium (web自动化工具) 拿到网页的源代码 --> 数据解析 --> 数据持久化

安装模块: pip install selenium 

导入: from selenium import webdriver

选中谷歌浏览器作为访问网站的浏览器(谷歌的驱动程序),根据谷歌浏览器版本下载对应的驱动程序:

版本和驱动程序: http://npm.taobao.org/mirrors/chromedriver/

谷歌版本查看: chrome://settings/help

下载好以后,将浏览器的驱动程序放到当前文件夹下(用相对路径)

交互: 用户对网页的操作

事件: 点击,键盘,鼠标移入移出(拖拽)

一定要先找到对应的标签,再去使用事件:

找标签: driver.find_element开头的方法

selenium适合做的爬虫场合: 模拟登陆